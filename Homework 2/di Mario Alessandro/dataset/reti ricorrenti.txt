Le Reti Neurali Ricorrenti (RNN) sono progettate per lavorare con dati sequenziali come serie temporali o testo. A differenza delle reti feedforward, le RNN hanno un meccanismo di memoria che permette all'output di un passo di influenzare l'input del passo successivo. Le varianti pi√π popolari, come LSTM e GRU, risolvono il problema della svanire del gradiente, cruciale nell'elaborazione del linguaggio naturale (NLP).