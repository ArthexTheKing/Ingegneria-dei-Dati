Il modello trasformatore, introdotto nel 2017, è l'architettura dominante in elaborazione del linguaggio naturale (NLP). A differenza delle RNN, esso si basa interamente sul meccanismo di attenzione (attention mechanism) per pesare l'importanza delle diverse parole nella sequenza di input. È composto da blocchi di encoder e decoder ed è alla base di modelli di grandi dimensioni come BERT e GPT. La sua capacità di parallelizzazione lo rende estremamente veloce.