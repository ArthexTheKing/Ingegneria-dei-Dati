La backpropagation (retropropagazione dell'errore) Ã¨ l'algoritmo chiave per addestrare le reti neurali. Dopo un forward pass che calcola l'output e la funzione di costo, il backward pass calcola il gradiente della funzione di costo rispetto ai pesi della rete. Questo gradiente indica la direzione e la magnitudine dell'errore e viene utilizzato da ottimizzatori (come l'Adam) per aggiornare i pesi e minimizzare l'errore.