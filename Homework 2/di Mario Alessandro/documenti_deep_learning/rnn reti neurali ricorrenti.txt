Le RNN (Recurrent Neural Networks) sono progettate per elaborare dati sequenziali come il testo o serie temporali. Hanno la capacit√† di mantenere uno 'stato' interno che rappresenta le informazioni passate. Tuttavia, soffrono del problema del vanishing gradient che impedisce di apprendere dipendenze a lungo termine. Per questo, le LSTM (Long Short-Term Memory) sono state introdotte come miglioramento, offrendo un meccanismo di gate per controllare il flusso di informazioni.